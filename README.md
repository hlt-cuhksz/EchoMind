<p align="center">
  <img src="figure/Logo.png" width="300"/>
</p>

# EchoMind: An Interrelated Multi‑Level Benchmark for Evaluating Empathetic Speech Language Models

<div align="center">
<a href="https://hlt-cuhksz.github.io/EchoMind/" target="_blank"><img src=https://img.shields.io/badge/Website-online-green.svg></a>
<a href="https://arxiv.org/abs/" target="_blank"><img src=https://img.shields.io/badge/arXiv-b5212f.svg?logo=arxiv></a>
<a href="https://huggingface.co/datasets/hlt-cuhksz/EchoMind" target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace%20Datasets-27b3b4.svg></a>
<a href="https://hlt-cuhksz.github.io/EchoMind/" target="_blank"><img src=https://img.shields.io/badge/%F0%9F%8F%86%20Leaderboard%20-27b333.svg></a>
</div>


## Overview
We introduce EchoMind, a benchmark designed to comprehensively assess the empathetic capabilities of Speech Language Models (SLMs) in dialogue scenarios.
Specifically, it evaluates their ability to perceive and incorporate non-lexical acoustic cues—beyond the spoken content—to infer speaker states and generate responses that are contextually and emotionally appropriate in text and vocal expressiveness.



<p align="center">
  <img src="figure/EchoMind.png"/>
</p>
 <figcaption>The EchoMind framework & examples. (a) Multi‑level cognitive process simulation for empathetic dialogue: Level 1—Understanding through content (ASR) and voice (MCQs); Level 2—Reasoning by integrating content and voice (MCQs); Level 3—Conversation with contextually and emotionally aligned responses (Open-domain Response). (b) Responses under controlled vocal-style variations of the same script—target, neutral, and alternative expressions—illustrating differences in response focus.</figcaption>
