<p align="center">
  <img src="figure/Logo.png" width="300"/>
</p>

# EchoMind: An Interrelated Multiâ€‘Level Benchmark for Evaluating Empathetic Speech Language Models

<div align="center">
<a href="https://hlt-cuhksz.github.io/EchoMind/" target="_blank"><img src=https://img.shields.io/badge/Website-online-green.svg></a>
<a href="https://arxiv.org/abs/" target="_blank"><img src=https://img.shields.io/badge/arXiv-b5212f.svg?logo=arxiv></a>
<a href="https://huggingface.co/datasets/hlt-cuhksz/EchoMind" target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace%20Datasets-27b3b4.svg></a>
<a href="https://hlt-cuhksz.github.io/EchoMind/" target="_blank"><img src=https://img.shields.io/badge/%F0%9F%8F%86%20Leaderboard%20-27b333.svg></a>
</div>


## ğŸ“– Overview
Speech Language Models (SLMs) have advanced spoken language understanding. However, it remains unclear whether they can truly hear youâ€”recognizing not only spoken words but also nonâ€‘lexical vocal cuesâ€”and respond with empathy, aligning replies both emotionally and contextually. Existing benchmarks typically evaluate linguistic, acoustic, reasoning, or dialogue abilities in isolation, overlooking the integration of these skills that is crucial for humanâ€‘like, emotionally intelligent conversation. We present EchoMind, the first interrelated, multiâ€‘stage benchmark that simulates the cognitive process of empathetic dialogue through sequential, contextâ€‘linked tasks: spokenâ€‘content understanding, vocalâ€‘cue perception, integrated reasoning, and response generation. All tasks share identical, semantically neutral scriptsâ€”free of explicit emotional or contextual cuesâ€”while controlled vocalâ€‘style variations test the effect of delivery independent of the transcript. EchoMind is grounded in an empathyâ€‘oriented framework spanning 3 coarse and 12 fineâ€‘grained dimensions, encompassing 39 vocal attributes, and evaluated using both objective and subjective metrics. Testing 12 advanced SLMs reveals that even stateâ€‘ofâ€‘theâ€‘art models struggle with high-expressive vocal cues, limiting empathetic response quality. Analyses of prompt strength, speech source, and ideal vocal cue recognition reveal persistent weaknesses in instructionâ€‘following, resilience to natural speech variability, and effective use of vocal cues for empathy. These results underscore the need for SLMs that integrate linguistic content with diverse vocal cues to achieve truly empathetic conversational ability.



<p align="center">
  <img src="figure/EchoMind.png"/>
</p>
 <figcaption>The EchoMind framework & examples. (a) Multiâ€‘level cognitive process simulation for empathetic dialogue: Level 1â€”Understanding through content (ASR) and voice (MCQs); Level 2â€”Reasoning by integrating content and voice (MCQs); Level 3â€”Conversation with contextually and emotionally aligned responses (Open-domain Response). (b) Responses under controlled vocal-style variations of the same scriptâ€”target, neutral, and alternative expressionsâ€”illustrating differences in response focus.</figcaption>


## ğŸš€ Getting Started
###  Setup
1. Clone the repository:
```bash
git clone https://github.com/hlt-cuhksz/EchoMind.git
cd EchoMind
```
2. download the dataset
```bash
git clone https://huggingface.co/datasets/hlt-cuhksz/EchoMind
mv EchoMind dataset
```

## ğŸ“ Project Structure

```
EchoMind/
â”œâ”€â”€ dataset
â”‚   â”œâ”€â”€ audio_response
â”‚   â”œâ”€â”€ data_human
â”‚   â”œâ”€â”€ data_synthesis
â”‚   â””â”€â”€ instruction
â”œâ”€â”€ figure
â”‚   â”œâ”€â”€ EchoMind.png
â”‚   â””â”€â”€ Logo.png
â”œâ”€â”€ requirements
â”‚   â”œâ”€â”€ audio-flamingo-3_setup_environment.sh
â”‚   â”œâ”€â”€ baichuan_omni_environment.yml
â”‚   â”œâ”€â”€ desta25_audio_setup_environment.sh
â”‚   â”œâ”€â”€ glm_environment.yml
â”‚   â”œâ”€â”€ gpt4o_environment.yml
â”‚   â”œâ”€â”€ kimi_audio_environment.yml
â”‚   â”œâ”€â”€ llama-omni2_environment.yml
â”‚   â”œâ”€â”€ opens2s_environment.yml
â”‚   â”œâ”€â”€ qwen25omni_environment.yml
â”‚   â”œâ”€â”€ speechgpt-audio-preview_setup_environment.sh
â”‚   â”œâ”€â”€ step-audio-chat_environment.txt
â”‚   â””â”€â”€ vita-audio_setup_environment.sh
â”œâ”€â”€ script
â”‚   â”œâ”€â”€ human_script
â”‚   â””â”€â”€ synthesis_scrip
â””â”€â”€ src
    â”œâ”€â”€ analysis-result
    â”œâ”€â”€ eval-result
    â”‚   â””â”€â”€ evaluation_metric.py
    â””â”€â”€ eval-slm
        â”œâ”€â”€ models
        â”œâ”€â”€ eval_asr_main.py
        â”œâ”€â”€ eval_mcq_main.py
        â”œâ”€â”€ eval_response_main.py
        â””â”€â”€ utils.py
```
