import json
import os
import tempfile
import sys
import re
import uuid
from argparse import ArgumentParser
from threading import Thread
from queue import Queue

import torchaudio
import torch
from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig, WhisperFeatureExtractor
from transformers.generation.streamers import BaseStreamer
from speech_tokenizer.modeling_whisper import WhisperVQEncoder

sys.path.insert(0, "./cosyvoice")
sys.path.insert(0, "./third_party/Matcha-TTS")

from speech_tokenizer.utils import extract_speech_token
from flow_inference import AudioDecoder
from audio_process import AudioStreamProcessor

audio_token_pattern = re.compile(r"<\|audio_(\d+)\|>")

class TokenStreamer(BaseStreamer):
    def __init__(self, skip_prompt: bool = False, timeout=None):
        self.skip_prompt = skip_prompt
        self.token_queue = Queue()
        self.stop_signal = None
        self.next_tokens_are_prompt = True
        self.timeout = timeout

    def put(self, value):
        if len(value.shape) > 1 and value.shape[0] > 1:
            raise ValueError("TextStreamer only supports batch size 1")
        elif len(value.shape) > 1:
            value = value[0]

        if self.skip_prompt and self.next_tokens_are_prompt:
            self.next_tokens_are_prompt = False
            return

        for token in value.tolist():
            self.token_queue.put(token)

    def end(self):
        self.token_queue.put(self.stop_signal)

    def __iter__(self):
        return self

    def __next__(self):
        value = self.token_queue.get(timeout=self.timeout)
        if value == self.stop_signal:
            raise StopIteration()
        else:
            return value

class ModelWorker:
    def __init__(self, model_path, dtype="bfloat16", device='cuda'):
        self.device = device
        self.bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        ) if dtype == "int4" else None

        print("æ­£åœ¨åŠ è½½GLMæ¨¡å‹...")
        self.glm_model = AutoModel.from_pretrained(
            model_path,
            trust_remote_code=True,
            quantization_config=self.bnb_config if self.bnb_config else None,
            device_map={"": 0}
        ).eval()
        self.glm_tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        print("GLMæ¨¡å‹åŠ è½½å®Œæˆ!")

    @torch.inference_mode()
    def generate_stream(self, params):
        tokenizer, model = self.glm_tokenizer, self.glm_model
        prompt = params["prompt"]
        temperature = float(params.get("temperature", 1.0))
        top_p = float(params.get("top_p", 1.0))
        max_new_tokens = int(params.get("max_new_tokens", 256))

        inputs = tokenizer([prompt], return_tensors="pt")
        inputs = inputs.to(self.device)
        streamer = TokenStreamer(skip_prompt=True)
        thread = Thread(
            target=model.generate,
            kwargs=dict(
                **inputs,
                max_new_tokens=int(max_new_tokens),
                temperature=float(temperature),
                top_p=float(top_p),
                streamer=streamer
            )
        )
        thread.start()
        for token_id in streamer:
            yield token_id

class GLM4VoiceInference:
    def __init__(self, 
                 model_path="/share/workspace/shared_models/01_LLM-models/glm-4-voice-9b",
                 flow_path="/share/workspace/shared_models/01_LLM-models/glm-4-voice-decoder",
                 tokenizer_path="/share/workspace/shared_models/01_LLM-models/glm-4-voice-tokenizer",
                 device="cuda",
                 dtype="bfloat16"):
        """
        åˆå§‹åŒ–GLM-4-Voiceæ¨ç†å™¨
        
        Args:
            model_path: GLMæ¨¡å‹è·¯å¾„
            flow_path: Flowæ¨¡å‹è·¯å¾„  
            tokenizer_path: Tokenizerè·¯å¾„
            device: è®¾å¤‡(cuda/cpu)
            dtype: æ•°æ®ç±»å‹(bfloat16/int4)
        """
        self.device = device
        self.flow_path = flow_path
        self.model_path = model_path
        self.tokenizer_path = tokenizer_path
        self.dtype = dtype
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_models()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ç»„ä»¶"""
        print("æ­£åœ¨åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ç»„ä»¶...")
        
        # åˆå§‹åŒ–GLMæ¨¡å‹å’Œtokenizer
        self.model_worker = ModelWorker(self.model_path, self.dtype, self.device)
        
        # Flow & Hift
        flow_config = os.path.join(self.flow_path, "config.yaml")
        flow_checkpoint = os.path.join(self.flow_path, 'flow.pt')
        hift_checkpoint = os.path.join(self.flow_path, 'hift.pt')
        
        print("æ­£åœ¨åŠ è½½éŸ³é¢‘è§£ç å™¨...")
        self.audio_decoder = AudioDecoder(
            config_path=flow_config, 
            flow_ckpt_path=flow_checkpoint,
            hift_ckpt_path=hift_checkpoint,
            device=self.device
        )
        
        # Speech tokenizer
        print("æ­£åœ¨åŠ è½½è¯­éŸ³tokenizer...")
        self.whisper_model = WhisperVQEncoder.from_pretrained(
            self.tokenizer_path
        ).eval().to(self.device)
        self.feature_extractor = WhisperFeatureExtractor.from_pretrained(
            self.tokenizer_path
        )
        
        print("æ‰€æœ‰æ¨¡å‹ç»„ä»¶åˆå§‹åŒ–å®Œæˆ!")
    
    def inference(self, 
                  audio_path=None,
                  text_input=None, 
                  output_dir="./outputs",
                  temperature=0.2,
                  top_p=0.8,
                  max_new_tokens=2000,
                  custom_system_prompt=None):
        """
        æ‰§è¡Œæ¨ç†
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„(å¯ä»¥ä¸text_inputåŒæ—¶ä½¿ç”¨)
            text_input: è¾“å…¥æ–‡æœ¬(å¯ä»¥ä¸audio_pathåŒæ—¶ä½¿ç”¨)
            output_dir: è¾“å‡ºç›®å½•
            temperature: æ¸©åº¦å‚æ•°
            top_p: top_på‚æ•°  
            max_new_tokens: æœ€å¤§æ–°tokenæ•°
            custom_system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿprompt
            
        Returns:
            dict: åŒ…å«è¾“å‡ºéŸ³é¢‘è·¯å¾„å’Œæ–‡æœ¬çš„å­—å…¸
        """
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # æ£€æŸ¥è¾“å…¥
        if audio_path is None and text_input is None:
            raise ValueError("å¿…é¡»æä¾›audio_pathæˆ–text_inputä¹‹ä¸€ï¼Œæˆ–è€…ä¸¤è€…éƒ½æä¾›")
        
        # ç¡®å®šè¾“å…¥æ¨¡å¼å’Œå¤„ç†è¾“å…¥
        user_input_parts = []
        
        if audio_path is not None and text_input is not None:
            # å¤šæ¨¡æ€è¾“å…¥ï¼šåŒæ—¶åŒ…å«éŸ³é¢‘å’Œæ–‡æœ¬
            input_mode = "multimodal"
            print(f"å¤„ç†å¤šæ¨¡æ€è¾“å…¥:")
            print(f"  éŸ³é¢‘: {audio_path}")
            print(f"  æ–‡æœ¬: {text_input}")
            
            # æå–éŸ³é¢‘tokens
            audio_tokens = extract_speech_token(
                self.whisper_model, self.feature_extractor, [audio_path]
            )[0]
            if len(audio_tokens) == 0:
                raise ValueError("æœªèƒ½ä»éŸ³é¢‘ä¸­æå–åˆ°tokens")
            audio_tokens_str = "".join([f"<|audio_{x}|>" for x in audio_tokens])
            audio_tokens_str = "<|begin_of_audio|>" + audio_tokens_str + "<|end_of_audio|>"
            
            # ç»„åˆéŸ³é¢‘å’Œæ–‡æœ¬è¾“å…¥
            user_input_parts.append(audio_tokens_str)
            user_input_parts.append(text_input)
            user_input = "\n".join(user_input_parts)
            
            default_system_prompt = "User will provide you with both speech and text instruction. Do it step by step. First, think about the instruction and respond in a interleaved manner, with 13 text token followed by 26 audio tokens."
            
        elif audio_path is not None:
            # ä»…éŸ³é¢‘è¾“å…¥
            input_mode = "audio"
            print(f"å¤„ç†éŸ³é¢‘è¾“å…¥: {audio_path}")
            # æå–éŸ³é¢‘tokens
            audio_tokens = extract_speech_token(
                self.whisper_model, self.feature_extractor, [audio_path]
            )[0]
            if len(audio_tokens) == 0:
                raise ValueError("æœªèƒ½ä»éŸ³é¢‘ä¸­æå–åˆ°tokens")
            audio_tokens_str = "".join([f"<|audio_{x}|>" for x in audio_tokens])
            audio_tokens_str = "<|begin_of_audio|>" + audio_tokens_str + "<|end_of_audio|>"
            user_input = audio_tokens_str
            
            default_system_prompt = "User will provide you with a speech instruction. Do it step by step. First, think about the instruction and respond in a interleaved manner, with 13 text token followed by 26 audio tokens."
        else:
            # ä»…æ–‡æœ¬è¾“å…¥
            input_mode = "text"  
            user_input = text_input
            print(f"å¤„ç†æ–‡æœ¬è¾“å…¥: {text_input}")
            default_system_prompt = "User will provide you with a text instruction. Do it step by step. First, think about the instruction and respond in a interleaved manner, with 13 text token followed by 26 audio tokens."
        
        # ä½¿ç”¨è‡ªå®šä¹‰ç³»ç»Ÿpromptæˆ–é»˜è®¤prompt
        system_prompt = custom_system_prompt if custom_system_prompt else default_system_prompt
        
        # æ„å»ºè¾“å…¥
        inputs = f"<|system|>\n{system_prompt}<|user|>\n{user_input}<|assistant|>streaming_transcription\n"
        
        print(f"å¼€å§‹æ¨ç†ï¼Œè¾“å…¥æ¨¡å¼: {input_mode}")
        
        # å‡†å¤‡æ¨ç†å‚æ•°
        params = {
            "prompt": inputs,
            "temperature": temperature,
            "top_p": top_p,
            "max_new_tokens": max_new_tokens,
        }
        
        # æ‰§è¡Œæ¨ç†
        with torch.no_grad():
            text_tokens, audio_tokens = [], []
            audio_offset = self.model_worker.glm_tokenizer.convert_tokens_to_ids('<|audio_0|>')
            end_token_id = self.model_worker.glm_tokenizer.convert_tokens_to_ids('<|user|>')
            complete_tokens = []
            prompt_speech_feat = torch.zeros(1, 0, 80).to(self.device)
            flow_prompt_speech_token = torch.zeros(1, 0, dtype=torch.int64).to(self.device)
            this_uuid = str(uuid.uuid4())
            tts_speechs = []
            tts_mels = []
            prev_mel = None
            is_finalize = False
            block_size_list = [25, 50, 100, 150, 200]
            block_size_idx = 0
            block_size = block_size_list[block_size_idx]
            audio_processor = AudioStreamProcessor()
            
            print("æ­£åœ¨ç”ŸæˆéŸ³é¢‘æµ...")
            
            # ç›´æ¥è°ƒç”¨æ¨¡å‹ç”Ÿæˆæµ
            for token_id in self.model_worker.generate_stream(params):
                if token_id == end_token_id:
                    is_finalize = True
                    
                if len(audio_tokens) >= block_size or (is_finalize and audio_tokens):
                    if block_size_idx < len(block_size_list) - 1:
                        block_size_idx += 1
                        block_size = block_size_list[block_size_idx]
                    
                    tts_token = torch.tensor(audio_tokens, device=self.device).unsqueeze(0)
                    
                    if prev_mel is not None:
                        prompt_speech_feat = torch.cat(tts_mels, dim=-1).transpose(1, 2)
                    
                    tts_speech, tts_mel = self.audio_decoder.token2wav(
                        tts_token, 
                        uuid=this_uuid,
                        prompt_token=flow_prompt_speech_token.to(self.device),
                        prompt_feat=prompt_speech_feat.to(self.device),
                        finalize=is_finalize
                    )
                    prev_mel = tts_mel
                    
                    tts_speechs.append(tts_speech.squeeze())
                    tts_mels.append(tts_mel)
                    flow_prompt_speech_token = torch.cat((flow_prompt_speech_token, tts_token), dim=-1)
                    audio_tokens = []
                
                if not is_finalize:
                    complete_tokens.append(token_id)
                    if token_id >= audio_offset:
                        audio_tokens.append(token_id - audio_offset)
                    else:
                        text_tokens.append(token_id)
        
        # åˆå¹¶æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
        if tts_speechs:
            tts_speech = torch.cat(tts_speechs, dim=-1).cpu()
        else:
            tts_speech = torch.zeros(1, 1)
            
        # è§£ç æ–‡æœ¬
        complete_text = self.model_worker.glm_tokenizer.decode(complete_tokens, spaces_between_special_tokens=False)
        text_only = self.model_worker.glm_tokenizer.decode(text_tokens, ignore_special_tokens=False)
        
        # ä¿å­˜è¾“å‡º
        timestamp = uuid.uuid4().hex[:8]
        
        # ä¿å­˜éŸ³é¢‘
        audio_output_path = os.path.join(output_dir, f"output_audio_{timestamp}.wav")
        torchaudio.save(audio_output_path, tts_speech.unsqueeze(0), 22050, format="wav")
        
        # ä¿å­˜æ–‡æœ¬
        text_output_path = os.path.join(output_dir, f"output_text_{timestamp}.txt")
        with open(text_output_path, 'w', encoding='utf-8') as f:
            f.write(f"å®Œæ•´è¾“å‡º:\n{complete_text}\n\n")
            f.write(f"çº¯æ–‡æœ¬éƒ¨åˆ†:\n{text_only}\n\n")
            f.write(f"è¾“å…¥æ¨¡å¼: {input_mode}\n")
            if input_mode == "audio":
                f.write(f"è¾“å…¥éŸ³é¢‘: {audio_path}\n")
            else:
                f.write(f"è¾“å…¥æ–‡æœ¬: {text_input}\n")
            f.write(f"æ¨ç†å‚æ•°:\n")
            f.write(f"  temperature: {temperature}\n")
            f.write(f"  top_p: {top_p}\n")
            f.write(f"  max_new_tokens: {max_new_tokens}\n")
        
        print(f"æ¨ç†å®Œæˆ!")
        print(f"éŸ³é¢‘è¾“å‡º: {audio_output_path}")
        print(f"æ–‡æœ¬è¾“å‡º: {text_output_path}")
        print(f"ç”Ÿæˆæ–‡æœ¬é¢„è§ˆ: {text_only[:100]}...")
        
        return {
            "audio_path": audio_output_path,
            "text_path": text_output_path,
            "complete_text": complete_text,
            "text_only": text_only
        }

def main():
    parser = ArgumentParser()
    parser.add_argument("--model-path", type=str, default="/share/workspace/shared_models/01_LLM-models/glm-4-voice-9b")
    parser.add_argument("--flow-path", type=str, default="/share/workspace/shared_models/01_LLM-models/glm-4-voice-decoder")
    parser.add_argument("--tokenizer-path", type=str, default="/share/workspace/shared_models/01_LLM-models/glm-4-voice-tokenizer")
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--dtype", type=str, default="bfloat16", choices=["bfloat16", "int4"])
    
    # è¾“å…¥å‚æ•°
    parser.add_argument("--audio-path", type=str, default=None, help="è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--text-input", type=str, default=None, help="è¾“å…¥æ–‡æœ¬å†…å®¹")
    parser.add_argument("--output-dir", type=str, default="./outputs", help="è¾“å‡ºç›®å½•")
    
    # æ¨ç†å‚æ•°
    parser.add_argument("--temperature", type=float, default=0.2)
    parser.add_argument("--top-p", type=float, default=0.8)  
    parser.add_argument("--max-new-tokens", type=int, default=2000)
    parser.add_argument("--system-prompt", type=str, default=None, help="è‡ªå®šä¹‰ç³»ç»Ÿprompt")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥
    if args.audio_path is None and args.text_input is None:
        print("é”™è¯¯: å¿…é¡»æä¾› --audio-path æˆ– --text-input ä¹‹ä¸€")
        return
    
    # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if args.audio_path and not os.path.exists(args.audio_path):
        print(f"é”™è¯¯: éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {args.audio_path}")
        return
    
    try:
        # åˆå§‹åŒ–æ¨ç†å™¨
        inferencer = GLM4VoiceInference(
            model_path=args.model_path,
            flow_path=args.flow_path,
            tokenizer_path=args.tokenizer_path,
            device=args.device,
            dtype=args.dtype
        )
        
        # æ‰§è¡Œæ¨ç†
        result = inferencer.inference(
            audio_path=args.audio_path,
            text_input=args.text_input,
            output_dir=args.output_dir,
            temperature=args.temperature,
            top_p=args.top_p,
            max_new_tokens=args.max_new_tokens,
            custom_system_prompt=args.system_prompt
        )
        
        print("\n" + "="*50)
        print("æ¨ç†ç»“æœ:")
        print(f"  éŸ³é¢‘æ–‡ä»¶: {result['audio_path']}")
        print(f"  æ–‡æœ¬æ–‡ä»¶: {result['text_path']}")
        print("="*50)
        
    except Exception as e:
        print(f"æ¨ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ğŸ§¹ æ¸…ç†GPUå†…å­˜...")
for i in range(torch.cuda.device_count()):
    with torch.cuda.device(i):
        torch.cuda.empty_cache()
    main()